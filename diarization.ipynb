{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "39018037-4312-44cc-8d5a-44cb11bfe126",
      "metadata": {
        "id": "39018037-4312-44cc-8d5a-44cb11bfe126"
      },
      "source": [
        "# Whisper Based Transcription and Speaker Diarization for YouTube Videos\n",
        "\n",
        "This notebook will leverage the OpenAI Whisper model and pyannote-audio to create a diarization system. \n",
        "\n",
        "## What is speaker diarization?\n",
        "\n",
        "Speaker diarization aims to answer the question of ***who spoke and when?***. \n",
        "In short: diarization algorithms break down an audio stream of multiple speakers into segments corresponding to the individual speakers. \n",
        "By combining the information that we get from diarization with Automated Speech Recognition (ASR) transcriptions, we can transform the generated \n",
        "transcript into a format which is more readable and interpretable for humans and that can be used for other downstream NLP tasks.\n",
        "\n",
        "<img src=\"https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/_images/asr_sd_diagram.png\" />\n",
        "\n",
        "An example output from an ASR system without diarization could be:\n",
        "\n",
        "```\n",
        "I just got back from the gym. oh good. uhuh. How's it going? oh pretty well. \n",
        "It was really crowded today yeah. I kind of assumed everyone would be at the shore.\n",
        "uhhuh. I was wrong. Well it's the middle of the week or whatever so. \n",
        "But it's the fourth of July. mm. So. yeah. People have to work tomorrow. Do you have to work tomorrow?\n",
        "yeah. Did youhave off yesterday? Yes. oh that's good. And I was paid too. oh. Is it paid today? No. oh.\n",
        "```\n",
        "\n",
        "With diarization, however, the conversation would become much more readable:\n",
        "\n",
        "```\n",
        "A: I just got back from the gym.\n",
        "B: oh good.\n",
        "A: uhhuh.\n",
        "B: How's it going?\n",
        "A: oh pretty well.\n",
        "A: It was really crowded today.\n",
        "B: yeah.\n",
        "A: I kind of assumed everyone would be at  the shore.\n",
        "B: uhhuh.\n",
        "A: I was wrong.\n",
        "B: Well it's the middle of the week or whatever so.\n",
        "A: But it's the fourth of July.\n",
        "B: mm.\n",
        "A: So.\n",
        "B: yeah.\n",
        "B: People have to work tomorrow.\n",
        "B: Do you have to work tomorrow?\n",
        "A: yeah.\n",
        "B: Did you have off yesterday?\n",
        "A: Yes.\n",
        "B: oh that's good.\n",
        "A: And I was paid too.\n",
        "B: oh.\n",
        "B: Is it paid today?\n",
        "A: No.\n",
        "B: oh.\n",
        "```\n",
        "\n",
        "Speaker-aware transcripts can be a powerful tool for analyzing speech data:\n",
        "\n",
        "* We can use the transcripts to analyze individual speaker's sentiment by using sentiment analysis on both audio and text transcripts.\n",
        "* Another use case is telemedicine where we might identify the `<doctor>`and `<patient>` tags on the transcription to create an accurate transcript and attach it to the patient file or EHR system.\n",
        "* Speaker diarization can be used by hiring platforms to analyze phone and video recruitment calls. This allows them to split and categorize candidates depending on their responses to certain questions without having to listen again to the records."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "G_n6vVmTZY1Z",
      "metadata": {
        "id": "G_n6vVmTZY1Z"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "To run this notebook you need a link to a YouTube video (`_YOUTUBE_VIDEO_URL`) and a HuggingFace authorization token (`_HF_AUTH_TOKEN`). You can create an account to HuggingFace services and acquire the authorization token by following [these instructions on the HuggingFace website](https://huggingface.co/docs/hub/security-tokens).\n",
        "\n",
        "Make sure you have a GPU runtime enabled by going to `Runtime -> Change runtime type` and make sure the `Hardware accelerator` is set to `GPU`. \n",
        "\n",
        "Insert the YouTube video URL to the following form. Execute the notebook by selecting `Runtime -> Run all` from the top dropdown menu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DdwVTgC-ZBFh",
      "metadata": {
        "cellView": "form",
        "id": "DdwVTgC-ZBFh"
      },
      "outputs": [],
      "source": [
        "\n",
        "_YOUTUBE_VIDEO_URL = \"https://www.youtube.com/watch?v=H7kZ98bAHaA\" #@param {type:\"string\"}\n",
        "_HF_AUTH_TOKEN = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "igEGpMvYYsO9",
      "metadata": {
        "id": "igEGpMvYYsO9"
      },
      "source": [
        "## Check GPU\n",
        "\n",
        "Check that we are running in an environment with a GPU available. Otherwise, the Whisper model is going to run very slowly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lqF03eTtmjrz",
      "metadata": {
        "cellView": "form",
        "id": "lqF03eTtmjrz"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import locale\n",
        "\n",
        "\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e0612a4-b9ed-45a1-bba0-5982c88542ab",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-04-11T12:35:23.972389Z",
          "iopub.status.busy": "2023-04-11T12:35:23.971368Z",
          "iopub.status.idle": "2023-04-11T12:35:24.728138Z",
          "shell.execute_reply": "2023-04-11T12:35:24.727304Z",
          "shell.execute_reply.started": "2023-04-11T12:35:23.972291Z"
        },
        "id": "3e0612a4-b9ed-45a1-bba0-5982c88542ab",
        "outputId": "5207cd58-0fa8-470c-8d90-6721cafc5a63"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95f1cf04-4efe-45e0-87d7-419aca12a66c",
      "metadata": {
        "id": "95f1cf04-4efe-45e0-87d7-419aca12a66c"
      },
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "959a7ade-3a1c-41a7-819e-643c6548437d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-04-11T12:35:27.159948Z",
          "iopub.status.busy": "2023-04-11T12:35:27.159623Z",
          "iopub.status.idle": "2023-04-11T12:37:22.961452Z",
          "shell.execute_reply": "2023-04-11T12:37:22.960506Z",
          "shell.execute_reply.started": "2023-04-11T12:35:27.159923Z"
        },
        "id": "959a7ade-3a1c-41a7-819e-643c6548437d",
        "outputId": "5fd65a79-0f11-460b-eeee-c025485e2209"
      },
      "outputs": [],
      "source": [
        "!apt update && apt install ffmpeg\n",
        "!apt install libvoikko1 libvoikko-dev voikko-fi python3-libvoikko --yes\n",
        "!pip install --upgrade \\\n",
        "    googletrans==4.0.0rc1 \\\n",
        "    yt-dlp \\\n",
        "    setuptools-rust \\\n",
        "    jiwer \\\n",
        "    pydub \\\n",
        "    pyannote.audio \\\n",
        "    git+https://github.com/openai/whisper.git \\\n",
        "    git+https://github.com/m-bain/whisperx.git \\\n",
        "    torch==1.12.1+cu116 \\\n",
        "    torchvision==0.13.1+cu116 \\\n",
        "    torchaudio==0.12.1 \\\n",
        "    --extra-index-url https://download.pytorch.org/whl/cu116\n",
        "!pip install torchtext==0.13.0 --force-reinstall --no-dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84605e87-197a-4f39-a453-4b2eb3bd0b46",
      "metadata": {
        "id": "84605e87-197a-4f39-a453-4b2eb3bd0b46"
      },
      "source": [
        "## Download YouTube video audio\n",
        "\n",
        "Download the audio for the selected YouTube video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "568fcc5d-218e-4d54-9e81-ac7b09f73efa",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-04-11T13:43:44.002205Z",
          "iopub.status.busy": "2023-04-11T13:43:44.001522Z",
          "iopub.status.idle": "2023-04-11T13:43:54.747236Z",
          "shell.execute_reply": "2023-04-11T13:43:54.746277Z",
          "shell.execute_reply.started": "2023-04-11T13:43:44.002174Z"
        },
        "id": "568fcc5d-218e-4d54-9e81-ac7b09f73efa",
        "outputId": "389a8f86-1341-4a52-d896-6c9297357cf8"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import re\n",
        "import os\n",
        "import yt_dlp\n",
        "\n",
        "\n",
        "def download_youtube_audio(yt_video_url: str) -> str:\n",
        "    yt_video_id = re.search(r\"v=([a-zA-Z0-9]*)\", yt_video_url).group(1)\n",
        "    output_file = f\"tmp/data/yt_audio/{yt_video_id}.wav\"\n",
        "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "\n",
        "    if os.path.exists(output_file):\n",
        "        return output_file\n",
        "\n",
        "    ydl_opts = {\n",
        "        \"format\": \"bestaudio/best\",\n",
        "        \"postprocessors\": [{\n",
        "            \"key\": \"FFmpegExtractAudio\",\n",
        "            \"preferredcodec\": \"wav\",\n",
        "            \"preferredquality\": \"192\",\n",
        "      }],\n",
        "        \"outtmpl\": \"tmp/data/yt_audio/%(id)s.%(ext)s\",\n",
        "    }\n",
        "\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        print(yt_video_url)\n",
        "        ydl.download([yt_video_url])\n",
        "    return output_file\n",
        "\n",
        "\n",
        "print(f\"Downloading YouTube audio for video: {_YOUTUBE_VIDEO_URL} ..\")\n",
        "audio_file_path = download_youtube_audio(_YOUTUBE_VIDEO_URL)\n",
        "print(f\"YouTube video audio downloaded successfully to: {audio_file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0ff537d-e9e2-4a42-b13e-9629acb14040",
      "metadata": {
        "id": "a0ff537d-e9e2-4a42-b13e-9629acb14040"
      },
      "source": [
        "## Optional: Crop the audio\n",
        "\n",
        "If the audio is very long we can crop the audio to a selected time interval `[T1, T2]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "343e36a7-7fe7-45cd-8f13-18d18622eb1a",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-04-11T13:44:01.909951Z",
          "iopub.status.busy": "2023-04-11T13:44:01.909281Z",
          "iopub.status.idle": "2023-04-11T13:44:02.622727Z",
          "shell.execute_reply": "2023-04-11T13:44:02.621899Z",
          "shell.execute_reply.started": "2023-04-11T13:44:01.909923Z"
        },
        "id": "343e36a7-7fe7-45cd-8f13-18d18622eb1a",
        "outputId": "af31bcdc-c9ce-492c-ab61-b367a01bdc91"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "from pydub import AudioSegment\n",
        "\n",
        "\n",
        "_AUDIO_CROP_T1_SEC = None\n",
        "_AUDIO_CROP_T2_SEC = None\n",
        "\n",
        "\n",
        "def crop_audio_file(\n",
        "    audio_file_path: str,\n",
        "    t1_sec: float,\n",
        "    t2_sec: float,\n",
        ") -> str:\n",
        "    if (t1_sec is None and t2_sec is None) or (t1_sec == 0.0 and t2_sec is None):\n",
        "        return audio_file_path\n",
        "    \n",
        "    assert(t1_sec < t2_sec)\n",
        "    output_file = f\"tmp/data/audio_crops/{os.path.basename(audio_file_path)}\"\n",
        "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "    \n",
        "    if os.path.exists(output_file):\n",
        "        return output_file\n",
        "\n",
        "    t1_ms = int(t1_sec * 1000)\n",
        "    t2_ms = int(t2_sec * 1000)\n",
        "    audio = AudioSegment.from_wav(audio_file_path)\n",
        "    cropped_audio = audio[t1_ms:min(t2_ms, len(audio))]\n",
        "    cropped_audio.export(output_file, format=\"wav\")\n",
        "    return output_file\n",
        "\n",
        "\n",
        "print(f\"Cropping the audio file to [{_AUDIO_CROP_T1_SEC}, {_AUDIO_CROP_T2_SEC}] sec..\")\n",
        "audio_crop_file_path = crop_audio_file(\n",
        "    audio_file_path=audio_file_path,\n",
        "    t1_sec=_AUDIO_CROP_T1_SEC,\n",
        "    t2_sec=_AUDIO_CROP_T2_SEC,\n",
        ")\n",
        "print(f\"Audio crop stored at: {audio_crop_file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18a00cd0-2205-4b3a-97fe-b175fbdbc8d9",
      "metadata": {
        "id": "18a00cd0-2205-4b3a-97fe-b175fbdbc8d9"
      },
      "source": [
        "## Speaker diarization with pyannote-audio\n",
        "\n",
        "We will use [pyannote-audio](https://github.com/pyannote/pyannote-audio) pretrained models to diarize the cropped audio file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e7c8240-9df3-4dce-a610-609b1513fbd0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-04-11T13:44:10.816428Z",
          "iopub.status.busy": "2023-04-11T13:44:10.815719Z",
          "iopub.status.idle": "2023-04-11T13:44:40.579204Z",
          "shell.execute_reply": "2023-04-11T13:44:40.578390Z",
          "shell.execute_reply.started": "2023-04-11T13:44:10.816401Z"
        },
        "id": "0e7c8240-9df3-4dce-a610-609b1513fbd0",
        "outputId": "7381fe97-6d6e-4f24-805b-0246e39ff525"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "from pyannote.audio import Pipeline\n",
        "\n",
        "\n",
        "try:\n",
        "    if pipeline:\n",
        "        print(\"Using existing diarization pipeline\")\n",
        "except NameError:\n",
        "    print(\"Creating new diarization pipeline\")\n",
        "    pipeline = Pipeline.from_pretrained(\n",
        "        \"pyannote/speaker-diarization@2.1\",\n",
        "        use_auth_token=_HF_AUTH_TOKEN,\n",
        "    )\n",
        "\n",
        "print(f\"Creating diarization for audio file: {audio_crop_file_path} ..\")\n",
        "diarization = pipeline(audio_crop_file_path)\n",
        "print(f\"Diarization complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N3WMC9vphB4L",
      "metadata": {
        "id": "N3WMC9vphB4L"
      },
      "source": [
        "Create diarization segments: in other words recognize who is speaking and at what time in the audio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bbad861-88e2-4de7-a499-cd105a0618a2",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-04-11T13:44:45.894457Z",
          "iopub.status.busy": "2023-04-11T13:44:45.893772Z",
          "iopub.status.idle": "2023-04-11T13:44:45.900332Z",
          "shell.execute_reply": "2023-04-11T13:44:45.899735Z",
          "shell.execute_reply.started": "2023-04-11T13:44:45.894430Z"
        },
        "id": "8bbad861-88e2-4de7-a499-cd105a0618a2",
        "outputId": "180f8eea-de56-4660-a5dd-8d1933ba3052"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DiarizationSegment:\n",
        "    start: float\n",
        "    end: float\n",
        "    speaker: str\n",
        "\n",
        "\n",
        "diarization_segments = [] \n",
        "print(f\"Diarization summary: {len(diarization)} segments with {len(diarization.labels())} unique speakers\")\n",
        "\n",
        "for idx, track in enumerate(diarization.itertracks(yield_label=True)):\n",
        "    turn, _, speaker = track\n",
        "    ds = DiarizationSegment(\n",
        "        start=turn.start,\n",
        "        end=turn.end,\n",
        "        speaker=speaker,\n",
        "    )\n",
        "    diarization_segments.append(ds)\n",
        "    print(f\"idx: {idx:03d}, start: {ds.start:06.1f}s, end: {ds.end:06.1f}s, speaker: {ds.speaker}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y7Da4d6chKRm",
      "metadata": {
        "id": "y7Da4d6chKRm"
      },
      "source": [
        "Postprocess the diarization segments. Combine two consecutive segments from the same speaker and fix problems with overlapping segments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e96d3b3-b582-4856-b852-912151df3dfd",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-04-11T13:44:55.826729Z",
          "iopub.status.busy": "2023-04-11T13:44:55.825931Z",
          "iopub.status.idle": "2023-04-11T13:44:55.833326Z",
          "shell.execute_reply": "2023-04-11T13:44:55.832618Z",
          "shell.execute_reply.started": "2023-04-11T13:44:55.826703Z"
        },
        "id": "1e96d3b3-b582-4856-b852-912151df3dfd",
        "outputId": "99158b26-dbc9-484a-f812-1455a9648f02"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import copy\n",
        "\n",
        "\n",
        "postprocessed_diarization_segments = []\n",
        "\n",
        "\n",
        "for idx, current_ds in enumerate(diarization_segments):\n",
        "    if idx == 0:\n",
        "        postprocessed_diarization_segments.append(copy.deepcopy(current_ds))\n",
        "        continue\n",
        "    \n",
        "    # If the current diarization segment overlaps with the previous one we need to split\n",
        "    # the segments into non overlapping parts\n",
        "    previous_ds = postprocessed_diarization_segments[-1]\n",
        "    \n",
        "    if current_ds.start < previous_ds.end:\n",
        "        previous_ds_original_end = previous_ds.end\n",
        "        \n",
        "        # Patch the previous to end at the start of current\n",
        "        previous_ds.end = current_ds.start\n",
        "        \n",
        "        # If the current segment is completely contained within the previous:\n",
        "        # Append two new segments\n",
        "        if current_ds.end < previous_ds_original_end:\n",
        "            postprocessed_diarization_segments.append(copy.deepcopy(current_ds))\n",
        "            postprocessed_diarization_segments.append(\n",
        "                DiarizationSegment(\n",
        "                    start=current_ds.end,\n",
        "                    end=previous_ds_original_end,\n",
        "                    speaker=previous_ds.speaker\n",
        "                )\n",
        "            )\n",
        "        # If the current ends after the previous segment:\n",
        "        # Only append the current segment\n",
        "        else:\n",
        "            postprocessed_diarization_segments.append(copy.deepcopy(current_ds))\n",
        "    else:\n",
        "        postprocessed_diarization_segments.append(copy.deepcopy(current_ds))\n",
        "\n",
        "        \n",
        "for idx, ds in enumerate(postprocessed_diarization_segments):\n",
        "    print(f\"idx: {idx:03d}, start: {ds.start:06.1f}s, end: {ds.end:06.1f}s, speaker: {ds.speaker}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DuvzuMXRor6B",
      "metadata": {
        "cellView": "form",
        "id": "DuvzuMXRor6B"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "_MAX_DIARIZATION_SEGMENT_LENGTH = None\n",
        "\n",
        "if _MAX_DIARIZATION_SEGMENT_LENGTH is not None and _MAX_DIARIZATION_SEGMENT_LENGTH > 0:\n",
        "    postprocessed_diarization_segments_2 = []\n",
        "\n",
        "    for idx, current_ds in enumerate(postprocessed_diarization_segments):\n",
        "        current_ds_length = current_ds.end - current_ds.start\n",
        "        if current_ds_length > _MAX_DIARIZATION_SEGMENT_LENGTH:\n",
        "            num_splits = int(current_ds_length / _MAX_DIARIZATION_SEGMENT_LENGTH)\n",
        "\n",
        "            for i in range(0, num_splits):\n",
        "                split_segment_start = current_ds.start + i * _MAX_DIARIZATION_SEGMENT_LENGTH\n",
        "                split_segment_end = split_segment_start + _MAX_DIARIZATION_SEGMENT_LENGTH\n",
        "                postprocessed_diarization_segments_2.append(\n",
        "                    DiarizationSegment(\n",
        "                        start=split_segment_start,\n",
        "                        end=split_segment_end,\n",
        "                        speaker=current_ds.speaker,\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            if current_ds_length % _MAX_DIARIZATION_SEGMENT_LENGTH != 0:\n",
        "                postprocessed_diarization_segments_2.append(\n",
        "                    DiarizationSegment(\n",
        "                        start=current_ds.start + num_splits * _MAX_DIARIZATION_SEGMENT_LENGTH,\n",
        "                        end=current_ds.end,\n",
        "                        speaker=current_ds.speaker,\n",
        "                    )\n",
        "                )\n",
        "\n",
        "    postprocessed_diarization_segments = postprocessed_diarization_segments_2\n",
        "    for idx, ds in enumerate(postprocessed_diarization_segments):\n",
        "        print(f\"idx: {idx:03d}, start: {ds.start:06.1f}s, end: {ds.end:06.1f}s, speaker: {ds.speaker}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac145ed3-cecb-4fe8-b593-967b9b00ac80",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-27T14:35:38.230301Z",
          "iopub.status.busy": "2023-01-27T14:35:38.229607Z",
          "iopub.status.idle": "2023-01-27T14:35:38.243233Z",
          "shell.execute_reply": "2023-01-27T14:35:38.242472Z",
          "shell.execute_reply.started": "2023-01-27T14:35:38.230272Z"
        },
        "id": "ac145ed3-cecb-4fe8-b593-967b9b00ac80"
      },
      "source": [
        "## Speech-to-text (STT) with Whisper\n",
        "\n",
        "Next we will do speech-to-text (STT) and optionally translation using the OpenAI Whisper model. \n",
        "Because the original Whisper model does not give accurate word level timestamps and the overall usage of the \n",
        "produced timestamps in the original Whisper model is quite convoluted we will use the model through the WhisperX module \n",
        "instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3373483c-61a4-436d-944e-33e2f495b85c",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-04-11T13:45:05.556311Z",
          "iopub.status.busy": "2023-04-11T13:45:05.555422Z",
          "iopub.status.idle": "2023-04-11T13:45:21.906957Z",
          "shell.execute_reply": "2023-04-11T13:45:21.906302Z",
          "shell.execute_reply.started": "2023-04-11T13:45:05.556285Z"
        },
        "id": "3373483c-61a4-436d-944e-33e2f495b85c",
        "outputId": "e2380c12-a069-4a03-d929-d235fa085141"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import whisper\n",
        "import torch\n",
        "\n",
        "\n",
        "_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # \"cuda\" or \"cpu\"\n",
        "_WHISPER_MODEL = \"large\" # \"tiny\", \"base\", \"small\", \"medium\", \"large\" + \".en\" for english only\n",
        "\n",
        "\n",
        "# Transcribe with the original Whisper model\n",
        "print(f\"Using device: {_DEVICE}\")\n",
        "\n",
        "try:\n",
        "    if whisper_model:\n",
        "        print(f\"Using existing Whisper model: {_WHISPER_MODEL}\")\n",
        "except NameError:\n",
        "    print(f\"Loading Whisper model: {_WHISPER_MODEL} ..\")\n",
        "    whisper_model = whisper.load_model(_WHISPER_MODEL, device=_DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a49cef43-ef7d-4d89-b794-15b3a6159203",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "91acf163760c4261833aace1e0c9ae58",
            "26e3e5bf7c814f9bad0c9388fb2afd25",
            "ab2567f59849437e96eeffabb01e4780",
            "ff54ca038ad2400495d54308fc5374c0",
            "f23d64a814ce42ea9f7e3a462b0ef45b",
            "0e5970c5cc274eeb84f66ab3e591ec16",
            "113b7ac5a25a4950a518d35a89d5c92e",
            "9858b5b2ff604026a100b8659fe0306a",
            "1df209ecd6ff4be99a0fabddcf5d8095",
            "1fa141c79ea64e608be99b64231b2e2b",
            "2a1d3b934e38438e95404713f2a5297d"
          ]
        },
        "execution": {
          "iopub.execute_input": "2023-04-11T13:45:29.302977Z",
          "iopub.status.busy": "2023-04-11T13:45:29.302695Z",
          "iopub.status.idle": "2023-04-11T13:48:59.100009Z",
          "shell.execute_reply": "2023-04-11T13:48:59.098993Z",
          "shell.execute_reply.started": "2023-04-11T13:45:29.302956Z"
        },
        "id": "a49cef43-ef7d-4d89-b794-15b3a6159203",
        "outputId": "dc1002bb-05fa-4ca1-e5da-1907a6553741"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import whisperx\n",
        "import torch\n",
        "import time\n",
        "import ffmpeg\n",
        "\n",
        "\n",
        "audio_length_sec = float(ffmpeg.probe(audio_crop_file_path)[\"format\"][\"duration\"])\n",
        "print(f\"Transcribing {audio_length_sec} seconds of audio: {audio_crop_file_path} ..\")\n",
        "s_time = time.time()\n",
        "transcription_result = whisper_model.transcribe(audio_crop_file_path)\n",
        "print(f\"Transcription completed in: {time.time()-s_time:.2f} seconds\")\n",
        "\n",
        "# Load alignment model and metadata\n",
        "print(f\"Loading alignment model for detected transcription language: {transcription_result['language']} ..\")\n",
        "align_model, align_model_metadata = whisperx.load_align_model(\n",
        "    language_code=transcription_result[\"language\"],\n",
        "    device=_DEVICE,\n",
        ")\n",
        "print(\"Aligning Whisper output segments ..\")\n",
        "s_time = time.time()\n",
        "transcription_result_aligned = whisperx.align(\n",
        "    transcription_result[\"segments\"],\n",
        "    align_model,\n",
        "    align_model_metadata,\n",
        "    audio_crop_file_path,\n",
        "    _DEVICE,\n",
        ")\n",
        "print(f\"Alignment completed in: {time.time()-s_time:.2f} seconds\")\n",
        "print(transcription_result_aligned[\"word_segments\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9c4c4af-4a28-4638-be07-55b19014e575",
      "metadata": {
        "id": "f9c4c4af-4a28-4638-be07-55b19014e575"
      },
      "source": [
        "## Combine transcription and diarization results\n",
        "\n",
        "Combine the the transcription and diarization results to get the final results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75dade14-10ed-4ba0-b766-2db1453698f3",
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2023-04-11T13:49:54.225761Z",
          "iopub.status.busy": "2023-04-11T13:49:54.225178Z",
          "iopub.status.idle": "2023-04-11T13:49:54.238580Z",
          "shell.execute_reply": "2023-04-11T13:49:54.237825Z",
          "shell.execute_reply.started": "2023-04-11T13:49:54.225727Z"
        },
        "id": "75dade14-10ed-4ba0-b766-2db1453698f3"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class SpeakerTranscribedSegment:\n",
        "    start: float\n",
        "    end: float\n",
        "    speaker: str\n",
        "    text: str\n",
        "\n",
        "  \n",
        "def combine_diarization_and_transcription(\n",
        "    diarization_segments,\n",
        "    transcription_result,\n",
        "    min_segment_length: float = 0.5,\n",
        "    use_postprocessing_heuristics: bool = False,\n",
        "    combine_consecutive_speaker_segments: bool = False,\n",
        ") -> List[SpeakerTranscribedSegment]:\n",
        "    \n",
        "    speaker_transcribed_segments = []\n",
        "    \n",
        "    transcription_segments = transcription_result[\"word_segments\"]\n",
        "    \n",
        "    current_ds_idx = 0\n",
        "    current_ds_ts_s_idx = 0\n",
        "    current_ds_ts_e_idx = 0\n",
        "    \n",
        "    while current_ds_idx < len(diarization_segments):\n",
        "        current_ds = diarization_segments[current_ds_idx]\n",
        "        \n",
        "        while current_ds_ts_e_idx < len(transcription_segments) and transcription_segments[current_ds_ts_e_idx][\"start\"] < current_ds.end:\n",
        "            current_ds_ts_e_idx += 1\n",
        "        \n",
        "        transcribed_segment_text = \" \".join(\n",
        "            [ts[\"text\"] for ts in transcription_segments[current_ds_ts_s_idx:current_ds_ts_e_idx]]\n",
        "        )\n",
        "        transcribed_segment_text = re.sub(' +', ' ', transcribed_segment_text).strip()\n",
        "        \n",
        "        # If the next segment speaker is the same as the previous modify the previous segment\n",
        "        # information and combine them together, do the same if the length of the segment is less\n",
        "        # than the minimum segment length\n",
        "        previous_speaker = None\n",
        "        \n",
        "        if len(speaker_transcribed_segments) > 0:\n",
        "            previous_speaker = speaker_transcribed_segments[-1].speaker\n",
        "        \n",
        "        if combine_consecutive_speaker_segments and ((previous_speaker == current_ds.speaker) or (current_ds.end - current_ds.start < min_segment_length)):\n",
        "                speaker_transcribed_segments[-1].end = current_ds.end\n",
        "                speaker_transcribed_segments[-1].text = speaker_transcribed_segments[-1].text + \" \" + transcribed_segment_text\n",
        "        else:\n",
        "            speaker_transcribed_segments.append(\n",
        "                SpeakerTranscribedSegment(\n",
        "                    start=current_ds.start,\n",
        "                    end=current_ds.end,\n",
        "                    speaker=current_ds.speaker,\n",
        "                    text=transcribed_segment_text,\n",
        "                )\n",
        "            )\n",
        "        \n",
        "        current_ds_ts_s_idx = current_ds_ts_e_idx\n",
        "        current_ds_ts_e_idx = current_ds_ts_e_idx + 1\n",
        "        current_ds_idx += 1\n",
        "    \n",
        "    # Try to fix small mistakes in transcribed diarization using heuristics\n",
        "    if use_postprocessing_heuristics:\n",
        "        for current_sts_idx, current_sts in enumerate(speaker_transcribed_segments):\n",
        "\n",
        "            # If the speaker transcribed segment does not begin with an upper case character there is\n",
        "            # likely a small mistake, since Whisper is very good at styling. Likely a part of the text\n",
        "            # belongs to the previous or next segment. Try to fix these small mistakes here.\n",
        "            if not current_sts.text[0].isupper():\n",
        "                max_words_to_move = 3\n",
        "                previous_sts = speaker_transcribed_segments[current_sts_idx-1]\n",
        "                first_sentence_in_current = re.split(r\"[\\?\\.]\", current_sts.text)[0]\n",
        "                last_sentence_in_previous = re.split(r\"[\\?\\.]\", previous_sts.text)[-1]\n",
        "\n",
        "                if len(last_sentence_in_previous.strip().split(\" \")) <= max_words_to_move:\n",
        "                    current_sts.text = last_sentence_in_previous.strip() + \" \" + current_sts.text\n",
        "                    previous_sts.text = previous_sts.text[0:-len(last_sentence_in_previous)].strip()\n",
        "                elif len(first_sentence_in_current.strip().split(\" \")) <= max_words_to_move:\n",
        "                    previous_sts.text = previous_sts.text + \" \" + first_sentence_in_current + \".\"\n",
        "                    current_sts.text = current_sts.text[len(first_sentence_in_current) + 2:-1].strip()\n",
        "\n",
        "        speaker_transcribed_segments = list([sts for sts in speaker_transcribed_segments if len(sts.text.strip()) > 0])\n",
        "                    \n",
        "    return speaker_transcribed_segments\n",
        "\n",
        "\n",
        "speaker_transcribed_segments = combine_diarization_and_transcription(\n",
        "    diarization_segments=postprocessed_diarization_segments,\n",
        "    transcription_result=transcription_result_aligned,\n",
        "    use_postprocessing_heuristics=True,\n",
        "    combine_consecutive_speaker_segments=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb0e5558-f342-4532-80e0-7aef74f4fb4a",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-04-11T13:50:00.643282Z",
          "iopub.status.busy": "2023-04-11T13:50:00.642701Z",
          "iopub.status.idle": "2023-04-11T13:50:00.648151Z",
          "shell.execute_reply": "2023-04-11T13:50:00.647143Z",
          "shell.execute_reply.started": "2023-04-11T13:50:00.643258Z"
        },
        "id": "eb0e5558-f342-4532-80e0-7aef74f4fb4a",
        "outputId": "42d17302-b39b-43a3-802c-b378e709af2a"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "for sts in speaker_transcribed_segments:\n",
        "    print(f\"[{sts.start:.2f}, {sts.end:.2f}] {sts.speaker}: {sts.text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B2hOxi03rFqP",
      "metadata": {
        "id": "B2hOxi03rFqP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "95f1cf04-4efe-45e0-87d7-419aca12a66c"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e5970c5cc274eeb84f66ab3e591ec16": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "113b7ac5a25a4950a518d35a89d5c92e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1df209ecd6ff4be99a0fabddcf5d8095": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1fa141c79ea64e608be99b64231b2e2b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26e3e5bf7c814f9bad0c9388fb2afd25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e5970c5cc274eeb84f66ab3e591ec16",
            "placeholder": "​",
            "style": "IPY_MODEL_113b7ac5a25a4950a518d35a89d5c92e",
            "value": "100%"
          }
        },
        "2a1d3b934e38438e95404713f2a5297d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91acf163760c4261833aace1e0c9ae58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_26e3e5bf7c814f9bad0c9388fb2afd25",
              "IPY_MODEL_ab2567f59849437e96eeffabb01e4780",
              "IPY_MODEL_ff54ca038ad2400495d54308fc5374c0"
            ],
            "layout": "IPY_MODEL_f23d64a814ce42ea9f7e3a462b0ef45b"
          }
        },
        "9858b5b2ff604026a100b8659fe0306a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab2567f59849437e96eeffabb01e4780": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9858b5b2ff604026a100b8659fe0306a",
            "max": 377664473,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1df209ecd6ff4be99a0fabddcf5d8095",
            "value": 377664473
          }
        },
        "f23d64a814ce42ea9f7e3a462b0ef45b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff54ca038ad2400495d54308fc5374c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fa141c79ea64e608be99b64231b2e2b",
            "placeholder": "​",
            "style": "IPY_MODEL_2a1d3b934e38438e95404713f2a5297d",
            "value": " 360M/360M [00:09&lt;00:00, 42.7MB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
